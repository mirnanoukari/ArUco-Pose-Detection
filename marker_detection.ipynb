{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "inputVideo = cv2.VideoCapture(0)\n",
    "#print(type(inputVideo))\n",
    "#check if connection with camera is successfully\n",
    "if inputVideo.isOpened():\n",
    "    ret, frame = inputVideo.read()  #capture a frame from live video\n",
    "\n",
    "print(inputVideo.isOpened())\n",
    "\n",
    "cameraMatrix = None\n",
    "distCoeffs = None\n",
    "markerLength = 0.05\n",
    "\n",
    "# You can read camera parameters from tutorial_camera_params.yml\n",
    "fs = cv2.FileStorage(\"tutorial_camera_params.yml\", cv2.FILE_STORAGE_READ)\n",
    "cameraMatrix = fs.getNode(\"cameraMatrix\").mat()\n",
    "distCoeffs = fs.getNode(\"distCoeffs\").mat()\n",
    "fs.release()\n",
    "\n",
    "# Set coordinate system\n",
    "objPoints = np.array([[-markerLength/2.0, markerLength/2.0, 0], [markerLength/2.0, markerLength/2.0, 0], [markerLength/2.0, -markerLength/2.0, 0], [-markerLength/2.0, -markerLength/2.0, 0]], dtype=np.float32)\n",
    "\n",
    "detectorParams = cv2.aruco.DetectorParameters()\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "\n",
    "while True:\n",
    "    ret, frame = inputVideo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(image, dictionary, parameters=detectorParams)\n",
    "    if ids is not None and len(ids) > 0:\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, markerLength, cameraMatrix, distCoeffs)\n",
    "        for rvec, tvec in zip(rvecs, tvecs):\n",
    "            cv2.drawFrameAxes(frame, cameraMatrix, distCoeffs, rvec, tvec, 0.1)\n",
    "    cv2.imshow(\"out\", frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "inputVideo.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "inputVideo = cv2.VideoCapture(\"video\\whiteBorders_Trim.mp4\")\n",
    "\n",
    "if inputVideo.isOpened():\n",
    "    ret, frame = inputVideo.read()\n",
    "\n",
    "print(inputVideo.isOpened())\n",
    "\n",
    "cameraMatrix = None\n",
    "distCoeffs = None\n",
    "markerLength = 0.05\n",
    "\n",
    "fs = cv2.FileStorage(\"tutorial_camera_params.yml\", cv2.FILE_STORAGE_READ)\n",
    "cameraMatrix = fs.getNode(\"cameraMatrix\").mat()\n",
    "distCoeffs = fs.getNode(\"distCoeffs\").mat()\n",
    "fs.release()\n",
    "\n",
    "detectorParams = cv2.aruco.DetectorParameters()\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "\n",
    "pose_data = pd.DataFrame(columns=['Time', 'Marker ID', 'tx', 'ty', 'tz', 'rx', 'ry', 'rz'])\n",
    "\n",
    "prev_positions = {}\n",
    "\n",
    "while True:\n",
    "    ret, frame = inputVideo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    height, width = frame.shape[:2]\n",
    "    scale = 0.5\n",
    "    scaled_frame = cv2.resize(frame, (int(width*scale), int(height*scale)))\n",
    "    image = cv2.cvtColor(scaled_frame, cv2.COLOR_BGR2GRAY)\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(image, dictionary, parameters=detectorParams)\n",
    "    if ids is not None and len(ids) > 0:\n",
    "        cv2.aruco.drawDetectedMarkers(scaled_frame, corners, ids)\n",
    "        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, markerLength, cameraMatrix, distCoeffs)\n",
    "        for id, rvec, tvec in zip(ids, rvecs, tvecs):\n",
    "            cv2.drawFrameAxes(scaled_frame, cameraMatrix, distCoeffs, rvec, tvec, 0.1)\n",
    "\n",
    "            if len(tvecs) == 3:\n",
    "                if tuple(id) in prev_positions:\n",
    "                    prev_t, prev_p, prev_o = prev_positions[tuple(id)]\n",
    "                    dt = pd.Timestamp.now() - prev_t\n",
    "                    v = (tvec - prev_p) / dt.total_seconds()\n",
    "                    p = tvec + v*dt.total_seconds()\n",
    "                    o = rvec + (rvec - prev_o) / dt.total_seconds()\n",
    "                else:\n",
    "                    p, o = tvec, rvec\n",
    "\n",
    "                intersection = np.mean(tvecs, axis=0)\n",
    "                print(ids, intersection)\n",
    "                rvec = np.zeros((3,1))\n",
    "                tvec = np.zeros((3,1))\n",
    "                imgpts, _ = cv2.projectPoints(intersection, rvec, tvec, cameraMatrix, distCoeffs)\n",
    "\n",
    "                # draw circle at intersection point\n",
    "                center = tuple(map(int, imgpts[0].ravel()))\n",
    "                cv2.circle(scaled_frame, center, 5, (0, 255, 0), -1)\n",
    "                #print('p:', p, 'o:', o)\n",
    "                data = {'Time': pd.Timestamp.now(), 'Marker ID': int(id), 'tx': p[0][0], 'ty': p[0][1], 'tz': p[0][2], 'rx': o[0][0], 'ry': o[0][1], 'rz': o[0][2]}\n",
    "                pose_data = pose_data.append(data, ignore_index=True)\n",
    "                prev_positions[tuple(id)] = (pd.Timestamp.now(), p, o)\n",
    "\n",
    "    cv2.imshow(\"out\", scaled_frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "pose_data.to_excel(\"pose_data.xlsx\", index=False)\n",
    "inputVideo.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tvecs [[[-0.43416566 -0.03145778  1.57225056]]\n",
      "\n",
      " [[-0.71812526  0.02275633  1.543125  ]]\n",
      "\n",
      " [[-0.51318101 -0.08877032  1.32194208]]]\n",
      "marker1 [[-0.43416566 -0.03145778  1.57225056]] marker2 [[-0.70251845  0.02528414  1.50230057]]\n",
      "ids:  [[13]\n",
      " [ 5]\n",
      " [11]] int:  [-0.56834206 -0.00308682  1.53727557]\n",
      "[[ 23741.16295461 -20467.29882362]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-c75009c38bba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m                     \u001b[0mintersection_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintersection_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Draw the detected aruco markers and axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "inputVideo = cv2.VideoCapture(\"video\\whiteBorders_Trim.mp4\")\n",
    "\n",
    "if inputVideo.isOpened():\n",
    "    ret, frame = inputVideo.read()\n",
    "\n",
    "print(inputVideo.isOpened())\n",
    "\n",
    "cameraMatrix = None\n",
    "distCoeffs = None\n",
    "markerLength = 0.05\n",
    "\n",
    "fs = cv2.FileStorage(\"tutorial_camera_params.yml\", cv2.FILE_STORAGE_READ)\n",
    "cameraMatrix = fs.getNode(\"cameraMatrix\").mat()\n",
    "distCoeffs = fs.getNode(\"distCoeffs\").mat()\n",
    "fs.release()\n",
    "\n",
    "detectorParams = cv2.aruco.DetectorParameters()\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "\n",
    "pose_data = pd.DataFrame(columns=['Time', 'Marker ID', 'tx', 'ty', 'tz', 'rx', 'ry', 'rz'])\n",
    "\n",
    "prev_positions = {}\n",
    "while True:\n",
    "    ret, frame = inputVideo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    height, width = frame.shape[:2]\n",
    "    scale = 0.5\n",
    "    scaled_frame = cv2.resize(frame, (int(width*scale), int(height*scale)))\n",
    "    image = cv2.cvtColor(scaled_frame, cv2.COLOR_BGR2GRAY)\n",
    "    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(image, dictionary, parameters=detectorParams)\n",
    "    if ids is not None and len(ids) > 0:\n",
    "        cv2.aruco.drawDetectedMarkers(scaled_frame, corners, ids)\n",
    "        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, markerLength, cameraMatrix, distCoeffs)\n",
    "        detected_ids = set(ids.flatten())\n",
    "        if 12 in detected_ids or 5 in detected_ids:  # check for required ids\n",
    "            for id, rvec, tvec in zip(ids, rvecs, tvecs):\n",
    "                cv2.drawFrameAxes(scaled_frame, cameraMatrix, distCoeffs, rvec, tvec, 0)\n",
    "                print(\"tvecs\",tvecs)\n",
    "                if len(tvecs) == 3:\n",
    "                    if tuple(id) in prev_positions:\n",
    "                        prev_t, prev_p, prev_o = prev_positions[tuple(id)]\n",
    "                        dt = pd.Timestamp.now() - prev_t\n",
    "                        v = (tvec - prev_p) / dt.total_seconds()\n",
    "                        p = tvec + v*dt.total_seconds()\n",
    "                        o = rvec + (rvec - prev_o) / dt.total_seconds()\n",
    "                    else:\n",
    "                        p, o = tvec, rvec\n",
    "\n",
    "                    # find the markers with required ids\n",
    "                    if id == 12 or id == 10 or id == 13:\n",
    "                        marker_1 = tvec \n",
    "                    if id == 5 or id == 8 or id == 6:\n",
    "                        marker_2 = tvec\n",
    "                        \n",
    "                    print(\"marker1\", marker_1,\"marker2\", marker_2)\n",
    "                    # fix z-axis as the common axis and find the intersection point\n",
    "                    marker_1_z = marker_1[:].reshape(-1)\n",
    "                    marker_2_z = marker_2[:].reshape(-1)\n",
    "                    # intersection = np.array([0, 0, (marker_1_z + marker_2_z) / 2])\n",
    "                    intersection = 0.5*(marker_1_z + marker_2_z)\n",
    "                    print(\"ids: \",ids, \"int: \", intersection)\n",
    "                    image_points = cv2.projectPoints(intersection, rvec, tvec, cameraMatrix, distCoeffs)[0][0]\n",
    "                    print(image_points)\n",
    "                    # Draw a circle at the intersection point (assuming we have an image called 'frame')\n",
    "                    intersection_x, intersection_y = int(intersection[0]), int(intersection[1])\n",
    "                    \n",
    "                    cv2.circle(scaled_frame, (int(image_points[0]),int(image_points[1])), 5, (255, 0, 255), -1)\n",
    "            \n",
    "        # Draw the detected aruco markers and axes\n",
    "        ee = cv2.aruco.drawDetectedMarkers(scaled_frame, corners, ids)\n",
    "        for i in range(len(ids)):\n",
    "            ee = cv2.drawFrameAxes(ee, cameraMatrix, distCoeffs, rvecs[i], tvecs[i], 0.1)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', ee)\n",
    "\n",
    "\n",
    "    # Exit loop if 'esc' key is pressed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "# Release the video capture object and close all windows\n",
    "inputVideo.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
